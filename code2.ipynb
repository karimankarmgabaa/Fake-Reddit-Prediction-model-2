{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d25c62c",
   "metadata": {},
   "source": [
    "<h1 style=\"property:value;color:Coral;font-size:300%;text-align:center; \">CISC-873-DM-F22-A3</h1> \n",
    "<h3 style=\"property:value;color:Coral;font-size:200%;text-align:center; \">Fake Reddit Prediction part2</h3> \n",
    "<h5 style=\"property:value;color:Sienna;font-size:150%; \">Kariman Karm Mohamed Mousaa </h5> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32633c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libararies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b190eea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In the 1920鈥檚, Hitler was forbidden to address...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nerd Wins Scrabble with word you've never hear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Why 95.8% of Female Newscasters Have the Same ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Donald Trump Says He'll Do This If More 'Inapp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5 crazy facts about Lamborghini's outrageous e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernie Sanders coming to the Walmart sharehold...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>James Corden reveals what he would do if Kanye...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Sorry R2-D2鈥?\"\\t\\t0\\t2\\t4\\nwelashubby\\tscient...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Granddaddy of something EVERYONE takes for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Minnesota: St. Paul launches web portal for Op...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Dem rep says he won't participate in moment of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Woman who loses everything in apartment fire f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Were these stories followed-up on? \"As White N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Oregon Eyewitness Blasts \"White Domestic Terro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"JULIA! WHERE'S YOUR PARACHUTE鈥?\"\\t\\t0\\t2\\t4\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"my wife left me two days ago鈥?\"\\t\\t0\\t2\\t4\\nD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Love This Story of a Day Spent With Maddy Stua...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>World鈥檚 first head transplant volunteer could ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"Fascist Infested!\" 1970 锟?\\t0.97\\t0\\t1\\t5\\nJo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"Oh, don't mind me鈥?\"\\t0.79\\t0\\t2\\t2\\nwuppinda...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A Terrible Ghost, 1944, \"Hitler can鈥檛 sleep. T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What the fuck did you just fucking say about m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>This Was Joe Budden's Response After Quavo Cal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What鈥檚 Behind The 鈥楥uckservative鈥?Slur? -- As ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Oxford Dictionaries鈥?word of the year is a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>H叹蜖虁蛝炭虙蜐蛡蛼虉虌蛦虇蛢蜎虥虈蛫虂叹虆蛺蛢虇虁虜虜蛦探蛻虝虈挞碳蜄台虧坍蛽挞蜌虩态抬瘫...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What the fuck did you just fucking say about m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>A woman in a restaurant complained about my ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>鈾?鈾?*\"Look around you. There are many things t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>鈥淪ay cheese!鈥?\"Wooah!鈥?\\t0.85\\t0\\t2\\t2\\nILikeN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>鈥淚 think it鈥檚 an important time in Mexico-US r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Howdy ya'll, my name is Kim John. I'm a 35 yea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>McCain Says that Trump and Congressional Repub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>T蜎虈虃停虌廷覊蹋蜋虧坦贪瘫蛿袒虦坛滩虦滩h停虂虓桐蛺桐虙虗虙虆虙虉虈彤虖蜔谭虥蛠覊蛪蹋碳蜋...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>\"Our battle has been long and drawn out... We'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Dovakhiin, Dovakhiin, naal ok zin los vahriin ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>**_艇蛻蛡虒坦虧虡虩碳摊虦虡_虂彤蛣停蛢蛺蛫虗虝蜑虩虡-蛬彤瞳庭虛艇蛡瞳虇蛠谈虡蛽虡胎'虄...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NEWS UPDATE: Trump disqualified from Republica...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I know who you are, I know what you want. You ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Warning: House of Cards season 2 Spoiler. Do n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>http://www.google.ca/imgres?um=1&amp;hl=en&amp;safe=of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Martha got home early from work. Her son was i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Next item is... This is item number 4 5 7 8 1 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>*\"Enemies of the Imperium, hear me. You have c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>The assignment was to take the hill. There wer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0   A group of friends began to volunteer at a hom...      0\n",
       "1   British Prime Minister @Theresa_May on Nerve A...      0\n",
       "2   In 1961, Goodyear released a kit that allows P...      0\n",
       "3   Happy Birthday, Bob Barker! The Price Is Right...      0\n",
       "4   Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0\n",
       "5   In the 1920鈥檚, Hitler was forbidden to address...      0\n",
       "6   Nerd Wins Scrabble with word you've never hear...      0\n",
       "7   Why 95.8% of Female Newscasters Have the Same ...      1\n",
       "8   Donald Trump Says He'll Do This If More 'Inapp...      0\n",
       "9   5 crazy facts about Lamborghini's outrageous e...      0\n",
       "10  Bernie Sanders coming to the Walmart sharehold...      0\n",
       "11  James Corden reveals what he would do if Kanye...      0\n",
       "12  \"Sorry R2-D2鈥?\"\\t\\t0\\t2\\t4\\nwelashubby\\tscient...      0\n",
       "13  The Granddaddy of something EVERYONE takes for...      1\n",
       "14  Minnesota: St. Paul launches web portal for Op...      0\n",
       "15  Dem rep says he won't participate in moment of...      0\n",
       "16  Woman who loses everything in apartment fire f...      0\n",
       "17  Were these stories followed-up on? \"As White N...      1\n",
       "18  Oregon Eyewitness Blasts \"White Domestic Terro...      0\n",
       "19  \"JULIA! WHERE'S YOUR PARACHUTE鈥?\"\\t\\t0\\t2\\t4\\n...      0\n",
       "20  \"my wife left me two days ago鈥?\"\\t\\t0\\t2\\t4\\nD...      0\n",
       "21  Love This Story of a Day Spent With Maddy Stua...      0\n",
       "22  World鈥檚 first head transplant volunteer could ...      0\n",
       "23  \"Fascist Infested!\" 1970 锟?\\t0.97\\t0\\t1\\t5\\nJo...      0\n",
       "24  \"Oh, don't mind me鈥?\"\\t0.79\\t0\\t2\\t2\\nwuppinda...      0\n",
       "25  A Terrible Ghost, 1944, \"Hitler can鈥檛 sleep. T...      0\n",
       "26  What the fuck did you just fucking say about m...      0\n",
       "27  This Was Joe Budden's Response After Quavo Cal...      0\n",
       "28  What鈥檚 Behind The 鈥楥uckservative鈥?Slur? -- As ...      1\n",
       "29  The Oxford Dictionaries鈥?word of the year is a...      0\n",
       "30  H叹蜖虁蛝炭虙蜐蛡蛼虉虌蛦虇蛢蜎虥虈蛫虂叹虆蛺蛢虇虁虜虜蛦探蛻虝虈挞碳蜄台虧坍蛽挞蜌虩态抬瘫...      0\n",
       "31  What the fuck did you just fucking say about m...      0\n",
       "32  A woman in a restaurant complained about my ki...      0\n",
       "33  鈾?鈾?*\"Look around you. There are many things t...      1\n",
       "34  鈥淪ay cheese!鈥?\"Wooah!鈥?\\t0.85\\t0\\t2\\t2\\nILikeN...      0\n",
       "35  鈥淚 think it鈥檚 an important time in Mexico-US r...      0\n",
       "36  Howdy ya'll, my name is Kim John. I'm a 35 yea...      0\n",
       "37  McCain Says that Trump and Congressional Repub...      1\n",
       "38  T蜎虈虃停虌廷覊蹋蜋虧坦贪瘫蛿袒虦坛滩虦滩h停虂虓桐蛺桐虙虗虙虆虙虉虈彤虖蜔谭虥蛠覊蛪蹋碳蜋...      0\n",
       "39  \"Our battle has been long and drawn out... We'...      0\n",
       "40  Dovakhiin, Dovakhiin, naal ok zin los vahriin ...      0\n",
       "41  **_艇蛻蛡虒坦虧虡虩碳摊虦虡_虂彤蛣停蛢蛺蛫虗虝蜑虩虡-蛬彤瞳庭虛艇蛡瞳虇蛠谈虡蛽虡胎'虄...      0\n",
       "42  NEWS UPDATE: Trump disqualified from Republica...      0\n",
       "43  I know who you are, I know what you want. You ...      0\n",
       "44  Warning: House of Cards season 2 Spoiler. Do n...      0\n",
       "45  http://www.google.ca/imgres?um=1&hl=en&safe=of...      0\n",
       "46  Martha got home early from work. Her son was i...      0\n",
       "47  Next item is... This is item number 4 5 7 8 1 ...      0\n",
       "48  *\"Enemies of the Imperium, hear me. You have c...      0\n",
       "49  The assignment was to take the hill. There wer...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data \n",
    "train=pd.read_csv('xy_train.csv')\n",
    "test=pd.read_csv('x_test.csv')\n",
    "sample=pd.read_csv('sample_submission.csv')\n",
    "ID=sample['id']\n",
    "#drop id \n",
    "train.drop('id',axis=1,inplace=True)\n",
    "test.drop('id',axis=1,inplace=True)\n",
    "train.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a331a2d",
   "metadata": {},
   "source": [
    "## Exploring data and Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137c782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    60000 non-null  object\n",
      " 1   label   60000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#information about train data \n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeed95be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61ff64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.467667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.506648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  60000.000000\n",
       "mean       0.467667\n",
       "std        0.506648\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        2.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#desribe the data\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29b0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to x,y\n",
    "x=train['text']\n",
    "y=train['label']\n",
    "test=test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c382be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     a group of friends began to volunteer at a hom...\n",
       "1     british prime minister theresamay on nerve att...\n",
       "2     in goodyear released a kit that allows to be b...\n",
       "3     happy birthday bob barker the price is right h...\n",
       "4     obama to nation 聙innocent cops and unarmed you...\n",
       "5     in the hitler was forbidden to address public ...\n",
       "6     nerd wins scrabble with word youve never heard...\n",
       "7     why of female newscasters have the same hair  ...\n",
       "8     donald trump says hell do this if more inappro...\n",
       "9     crazy facts about lamborghinis outrageous elec...\n",
       "10    bernie sanders coming to the walmart sharehold...\n",
       "11    james corden reveals what he would do if kanye...\n",
       "12    sorry welashubby scientists discover that fart...\n",
       "13    the granddaddy of something everyone takes for...\n",
       "14    minnesota st paul launches web portal for oppo...\n",
       "15    dem rep says he wont participate in moment of ...\n",
       "16    woman who loses everything in apartment fire f...\n",
       "17    were these stories followedup on as white nati...\n",
       "18    oregon eyewitness blasts white domestic terror...\n",
       "19    julia wheres your parachute鈥  frenchymanfella ...\n",
       "20    my wife left me two days ago鈥  s national soci...\n",
       "21    love this story of a day spent with maddy stua...\n",
       "22    world鈥檚 first head transplant volunteer could ...\n",
       "23    fascist infested 锟 jodieboo husky and a rubber...\n",
       "24    oh dont mind me鈥 wuppindalsa cookie monster ex...\n",
       "25    a terrible ghost hitler can鈥檛 sleep through th...\n",
       "26    what the fuck did you just fucking say about m...\n",
       "27    this was joe buddens response after quavo call...\n",
       "28    what鈥檚 behind the 鈥楥uckservative鈥slur  as eric...\n",
       "29    the oxford dictionaries鈥word of the year is a ...\n",
       "30    h叹蜖虁蛝炭虙蜐蛡蛼虉虌蛦虇蛢蜎虥虈蛫虂叹虆蛺蛢虇虁虜虜蛦探蛻虝虈挞碳蜄台虧坍蛽挞蜌虩态抬瘫...\n",
       "31    what the fuck did you just fucking say about m...\n",
       "32    a woman in a restaurant complained about my ki...\n",
       "33    鈾鈾look around you there are many things to see...\n",
       "34    鈥淪ay cheese鈥wooah鈥 ilikeneurons gop slowly piv...\n",
       "35    鈥淚 think it鈥檚 an important time in mexicous re...\n",
       "36    howdy yall my name is kim john im a year old k...\n",
       "37    mccain says that trump and congressional repub...\n",
       "38    t蜎虈虃停虌廷覊蹋蜋虧坦贪瘫蛿袒虦坛滩虦滩h停虂虓桐蛺桐虙虗虙虆虙虉虈彤虖蜔谭虥蛠覊蛪蹋碳蜋...\n",
       "39    our battle has been long and drawn out weve su...\n",
       "40    dovakhiin dovakhiin naal ok zin los vahriin wa...\n",
       "41    艇蛻蛡虒坦虧虡虩碳摊虦虡虂彤蛣停蛢蛺蛫虗虝蜑虩虡蛬彤瞳庭虛艇蛡瞳虇蛠谈虡蛽虡胎虄亭叹虌坦贪蛵...\n",
       "42    news update trump disqualified from republican...\n",
       "43    i know who you are i know what you want you ar...\n",
       "44    warning house of cards season spoiler do not c...\n",
       "45                                                     \n",
       "46    martha got home early from work her son was in...\n",
       "47    next item is this is item number look at these...\n",
       "48    enemies of the imperium hear me you have come ...\n",
       "49    the assignment was to take the hill there were...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Libraries regular expression and string \n",
    "import re\n",
    "import string\n",
    "#function for clean text \n",
    "#in trail1,2,3 \n",
    "def clean_text(text):\n",
    "    # covert the text to lowercase\n",
    "    text = text.lower()  \n",
    "    # remove html chars\n",
    "    text = re.sub('<.*?>','',text).strip() \n",
    "    # remove text in square brackets and parenthesis\n",
    "    text = re.sub('\\[|\\(.*\\]|\\)','', text).strip() \n",
    "     # remove punctuation marks\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # remove non-ascii chars\n",
    "    text = re.sub(\"(\\\\W)\",\" \",text).strip() \n",
    "    # remove words containing numbers\n",
    "    text = re.sub('\\S*\\d\\S*\\s*','', text).strip()  \n",
    "    #remove non english characters \n",
    "    #text =  re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "    return text\n",
    "#covert type of series to string and apply the function for each column\n",
    "x = x.astype(str)\n",
    "x = x.apply(clean_text)\n",
    "test=test.astype(str)\n",
    "test=test.apply(clean_text)\n",
    "x.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d70246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ec0acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the english language small model of spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#x = x.astype(str)\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee934f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    group friend began volunt homeless shelter nei...\n",
       "1    british prime minist theresamay nerv attack ru...\n",
       "2    goodyear releas kit allow brought heel 鈥 fish ...\n",
       "3    happi birthday bob barker price right host hed...\n",
       "4    obama nation 聙innoc cop unarm young black men ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function remove stop words from a text.\n",
    "def apply_stemmer(text):\n",
    "    words = text.split()\n",
    "    sent = [snow_stemmer.stem(word) for word in words if not word in set(stopwords)]\n",
    "    return ' '.join(sent)\n",
    "# remove the stop words from x and test\n",
    "x = x.apply(apply_stemmer)\n",
    "test=test.apply(apply_stemmer)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35159abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "like        3086\n",
       "new         2986\n",
       "look        2836\n",
       "color       2622\n",
       "man         2596\n",
       "trump       2370\n",
       "peopl       2257\n",
       "found       1994\n",
       "time        1951\n",
       "poster      1900\n",
       "day         1806\n",
       "war         1767\n",
       "work        1498\n",
       "say         1487\n",
       "help        1440\n",
       "world       1430\n",
       "life        1393\n",
       "american    1385\n",
       "old         1372\n",
       "state       1348\n",
       "school      1313\n",
       "photo       1308\n",
       "save        1295\n",
       "know        1250\n",
       "hous        1224\n",
       "want        1214\n",
       "circa       1211\n",
       "presid      1206\n",
       "right       1201\n",
       "psbattl     1164\n",
       "woman       1161\n",
       "pictur      1158\n",
       "way         1130\n",
       "true        1120\n",
       "got         1119\n",
       "find        1100\n",
       "get         1094\n",
       "polic       1080\n",
       "dog         1067\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.models import NumeralTickFormatter\n",
    "# Word Frequency of most common words\n",
    "word_freq = pd.Series(\" \".join(x).split()).value_counts()\n",
    "word_freq[1:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "573dd426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>鈥淕rosvenor鈥metro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>circumfer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wbush</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>homefront</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>congol茅opoldvill</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>napoleon鈥檚</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>out鈥</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>soborova</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tapes鈥a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index  freq\n",
       "0  鈥淕rosvenor鈥metro     1\n",
       "1         circumfer     1\n",
       "2              aang     1\n",
       "3             wbush     1\n",
       "4         homefront     1\n",
       "5  congol茅opoldvill     1\n",
       "6        napoleon鈥檚     1\n",
       "7              out鈥     1\n",
       "8          soborova     1\n",
       "9           tapes鈥a     1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list most uncommon words\n",
    "word_freq[-10:].reset_index(name=\"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5c6bd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dad vietnam didnt know photo exist came random internet cri saw hope right place post             0.000433\n",
       "rfakehistoryporn subscrib attempt read seri basic instruct                                        0.000367\n",
       "dad vietnam didn鈥檛 know photo exist came random internet cri saw hope right place post            0.000300\n",
       "                                                                                                  0.000133\n",
       "new evid confirm human ancestor climb tree retriev drop snack                                     0.000050\n",
       "                                                                                                    ...   \n",
       "warrior attribut final loss durant鈥檚 ruptur achill klay鈥檚 torn acl curri hit bus near end game    0.000017\n",
       "peopl walk brother post peopl add thing wed album help epic                                       0.000017\n",
       "ed gein butcher plainsfield pictur mask skin femal head circa                                     0.000017\n",
       "german soldier enjoy rare christma meal sent ruin stalingrad januari color                        0.000017\n",
       "jeff bridg releas 鈥楽leep tapes鈥a new album design help fall asleep                                0.000017\n",
       "Name: text, Length: 59563, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of ratings\n",
    "x.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c265b18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "# Sample data - 20% of data to validation set\n",
    "x_train,x_val,y_train,y_val = train_test_split(x,y, random_state=1, test_size=0.2, shuffle=True)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7fdb87",
   "metadata": {},
   "source": [
    "## LogisticRegression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2334c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier Pipeline\n",
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"logistic\", LogisticRegression())])\n",
    "# Params for classifier\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 3)],\n",
    "    \"tfidf__max_df\": [0.5],\n",
    "    \"tfidf__min_df\": [5],\n",
    "    \"logistic__C\": np.arange(0.2, 1, 0.15),\n",
    "    \"logistic__random_state\":[100]\n",
    "}\n",
    "\n",
    "# DEFINE PARAMETER c take values logspace(-3,3,7) and penalty  l2 ridge\n",
    "#grid={\"C\":np.logspace(-3,3,20), \"penalty\":[\"l2\"],'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n",
    "# Grid Search Execute\n",
    "rf_grid = RandomizedSearchCV(pipe, params, n_jobs=-1, scoring=\"f1_macro\",  n_iter=3)\n",
    "rf_detector = rf_grid.fit(x_train, y_train)\n",
    "print(\"train accuracy: \",accuracy_score(rf_detector, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52c2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5a61763",
   "metadata": {},
   "source": [
    "## Random Forest Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8725f194",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter svc for estimator Pipeline(steps=[('tfidf', TfidfVectorizer()),\n                ('classifier', RandomForestClassifier())]). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"D:\\conda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"D:\\conda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"D:\\conda\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"D:\\conda\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"D:\\conda\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"D:\\conda\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 222, in __call__\n    return self.function(*args, **kwargs)\n  File \"D:\\conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 586, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"D:\\conda\\lib\\site-packages\\sklearn\\pipeline.py\", line 150, in set_params\n    self._set_params('steps', **kwargs)\n  File \"D:\\conda\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 54, in _set_params\n    super().set_params(**params)\n  File \"D:\\conda\\lib\\site-packages\\sklearn\\base.py\", line 230, in set_params\n    raise ValueError('Invalid parameter %s for estimator %s. '\nValueError: Invalid parameter svc for estimator Pipeline(steps=[('tfidf', TfidfVectorizer()),\n                ('classifier', RandomForestClassifier())]). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SAIFTO~1\\AppData\\Local\\Temp/ipykernel_8552/990707819.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Grid Search Execute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mrf_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"f1_macro\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mrf_detector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_detector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1631\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1633\u001b[1;33m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[0;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m             random_state=self.random_state))\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                 \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter svc for estimator Pipeline(steps=[('tfidf', TfidfVectorizer()),\n                ('classifier', RandomForestClassifier())]). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    " #Classifier Pipeline\n",
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"classifier\", RandomForestClassifier())])\n",
    "# Params for classifier\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 3)],\n",
    "    \"tfidf__max_df\": [0.5],\n",
    "    \"tfidf__min_df\": [5],\n",
    "    \"svc__C\": np.arange(0.2, 1, 0.15),\n",
    "}\n",
    "\n",
    "# DEFINE PARAMETER c take values logspace(-3,3,7) and penalty  l2 ridge\n",
    "#grid={\"C\":np.logspace(-3,3,20), \"penalty\":[\"l2\"],'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n",
    "# Grid Search Execute\n",
    "rf_grid = RandomizedSearchCV(pipe, params, n_jobs=-1, scoring=\"f1_macro\",  n_iter=3)\n",
    "rf_detector = rf_grid.fit(x_train, y_train)\n",
    "print(\"train accuracy: \",accuracy_score(rf_detector, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cab9757e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Randomforest',\n",
       " 'Randomforest__bootstrap',\n",
       " 'Randomforest__ccp_alpha',\n",
       " 'Randomforest__class_weight',\n",
       " 'Randomforest__criterion',\n",
       " 'Randomforest__max_depth',\n",
       " 'Randomforest__max_features',\n",
       " 'Randomforest__max_leaf_nodes',\n",
       " 'Randomforest__max_samples',\n",
       " 'Randomforest__min_impurity_decrease',\n",
       " 'Randomforest__min_impurity_split',\n",
       " 'Randomforest__min_samples_leaf',\n",
       " 'Randomforest__min_samples_split',\n",
       " 'Randomforest__min_weight_fraction_leaf',\n",
       " 'Randomforest__n_estimators',\n",
       " 'Randomforest__n_jobs',\n",
       " 'Randomforest__oob_score',\n",
       " 'Randomforest__random_state',\n",
       " 'Randomforest__verbose',\n",
       " 'Randomforest__warm_start',\n",
       " 'memory',\n",
       " 'steps',\n",
       " 'tfidf',\n",
       " 'tfidf__analyzer',\n",
       " 'tfidf__binary',\n",
       " 'tfidf__decode_error',\n",
       " 'tfidf__dtype',\n",
       " 'tfidf__encoding',\n",
       " 'tfidf__input',\n",
       " 'tfidf__lowercase',\n",
       " 'tfidf__max_df',\n",
       " 'tfidf__max_features',\n",
       " 'tfidf__min_df',\n",
       " 'tfidf__ngram_range',\n",
       " 'tfidf__norm',\n",
       " 'tfidf__preprocessor',\n",
       " 'tfidf__smooth_idf',\n",
       " 'tfidf__stop_words',\n",
       " 'tfidf__strip_accents',\n",
       " 'tfidf__sublinear_tf',\n",
       " 'tfidf__token_pattern',\n",
       " 'tfidf__tokenizer',\n",
       " 'tfidf__use_idf',\n",
       " 'tfidf__vocabulary',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(pipeline.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1edb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
